FROM /Volumes/kuma/LLM_Models_here/llama-3.2-Korean-Bllossom-3B.Q8_0.gguf

# sets the temperature to 1 [higher is more creative, lower is more coherent]
PARAMETER temperature 0.25
# sets the context window size to 4096, this controls how many tokens the LLM can use as context to generate the next token
PARAMETER num_ctx 4096

TEMPLATE """{{ if .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{ end }}{{ if .Prompt }}<|im_start|>user
{{ .Prompt }}<|im_end|>
{{ end }}<|im_start|>assistant
"""

# sets a custom system message to specify the behavior of the chat assistant
SYSTEM You are acting as an assistant.
