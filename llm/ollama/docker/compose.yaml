# ./docker-compose.yml
version: "3.9"

name: ollama-stack

services:
  ollama:
    # 로컬 Dockerfile 사용(또는 image: ollama/ollama:latest 로 대체 가능)
    build:
      context: .
      dockerfile: ./docker/ollama/Dockerfile
    # image: ollama/ollama:latest   # ← 빌드 대신 공식 이미지 그대로 쓰려면 주석 해제
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama           # 모델/캐시 보존
    environment:
      # 외부 접속 허용 및 CORS (프론트엔드 도메인에 맞게 수정 권장)
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=http://localhost:3000,https://your-frontend.example
      # (선택) 튜닝 옵션 예시
      # - OLLAMA_KEEP_ALIVE=24h
      # - OLLAMA_MAX_LOADED_MODELS=2
      # - OLLAMA_NUM_PARALLEL=2
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:11434/api/tags"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 15s
    init: true
    restart: unless-stopped

    # ===== GPU 사용 시 =====
    # 'docker compose --profile gpu up -d' 로 활성화
    profiles: ["cpu"]
    # 아래 gpu 항목은 gpu 프로필 서비스에서 덮어씌움

  # GPU 프로필용 동일 서비스(간단하고 명시적인 방식)
  ollama-gpu:
    extends:
      service: ollama
    container_name: ollama-gpu
    profiles: ["gpu"]
    # Docker 24+/Compose v2에서 공식 지원
    gpus: all
    # NVIDIA 드라이버/툴킷이 호스트에 설치되어 있어야 함 (nvidia-container-toolkit)

  # 초기 모델 프리풀(one-shot). 필요 모델을 여기서 지정.
  init-models:
    image: ollama/ollama:latest
    profiles: ["cpu", "gpu"]
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      # init 컨테이너가 메인 서비스에 붙어 pull 수행
      - OLLAMA_HOST=http://ollama:11434
    volumes:
      - ollama_data:/root/.ollama
    # 원하는 모델을 공백으로 구분해 나열
    # 예시: llama3.1:8b-instruct, qwen2.5:7b-instruct, mistral:7b-instruct
    command: >
      /bin/sh -lc "
        set -e;
        for m in
          llama3.1:8b-instruct
          qwen2.5:7b-instruct
        ; do
          echo '==> pulling' $$m;
          ollama pull $$m;
        done;
        echo 'all models pulled.'
      "
    restart: "no"

volumes:
  ollama_data:
    driver: local
